[
  {"id":"chenSimpleFrameworkContrastive2020a","abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenSimpleFrameworkContrastive2020a","container-title":"arXiv:2002.05709 [cs, stat]","issued":{"date-parts":[[2020,6,30]]},"source":"arXiv.org","title":"A Simple Framework for Contrastive Learning of Visual Representations","type":"article-journal","URL":"http://arxiv.org/abs/2002.05709"},
  {"id":"heMomentumContrastUnsupervised2020","abstract":"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"He","given":"Kaiming"},{"family":"Fan","given":"Haoqi"},{"family":"Wu","given":"Yuxin"},{"family":"Xie","given":"Saining"},{"family":"Girshick","given":"Ross"}],"citation-key":"heMomentumContrastUnsupervised2020","container-title":"arXiv:1911.05722 [cs]","issued":{"date-parts":[[2020,3,23]]},"source":"arXiv.org","title":"Momentum Contrast for Unsupervised Visual Representation Learning","type":"article-journal","URL":"http://arxiv.org/abs/1911.05722"},
  {"id":"henaffDataEfficientImageRecognition2020","abstract":"Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"HÃ©naff","given":"Olivier J."},{"family":"Srinivas","given":"Aravind"},{"family":"De Fauw","given":"Jeffrey"},{"family":"Razavi","given":"Ali"},{"family":"Doersch","given":"Carl"},{"family":"Eslami","given":"S. M. Ali"},{"family":"Oord","given":"Aaron","dropping-particle":"van den"}],"citation-key":"henaffDataEfficientImageRecognition2020","container-title":"arXiv:1905.09272 [cs]","issued":{"date-parts":[[2020,7,1]]},"source":"arXiv.org","title":"Data-Efficient Image Recognition with Contrastive Predictive Coding","type":"article-journal","URL":"http://arxiv.org/abs/1905.09272"},
  {"id":"le-khacContrastiveRepresentationLearning2020","abstract":"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Le-Khac","given":"Phuc H."},{"family":"Healy","given":"Graham"},{"family":"Smeaton","given":"Alan F."}],"citation-key":"le-khacContrastiveRepresentationLearning2020","container-title":"IEEE Access","container-title-short":"IEEE Access","DOI":"10.1109/ACCESS.2020.3031549","ISSN":"2169-3536","issued":{"date-parts":[[2020]]},"page":"193907-193934","source":"arXiv.org","title":"Contrastive Representation Learning: A Framework and Review","title-short":"Contrastive Representation Learning","type":"article-journal","URL":"http://arxiv.org/abs/2010.05113","volume":"8"},
  {"id":"tianContrastiveMultiviewCoding2020","abstract":"Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a \"dog\" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tian","given":"Yonglong"},{"family":"Krishnan","given":"Dilip"},{"family":"Isola","given":"Phillip"}],"citation-key":"tianContrastiveMultiviewCoding2020","container-title":"arXiv:1906.05849 [cs]","issued":{"date-parts":[[2020,12,18]]},"source":"arXiv.org","title":"Contrastive Multiview Coding","type":"article-journal","URL":"http://arxiv.org/abs/1906.05849"},
  {"id":"tianDivideContrastSelfsupervised2021","abstract":"Self-supervised learning holds promise in leveraging large amounts of unlabeled data, however much of its progress has thus far been limited to highly curated pre-training data such as ImageNet. We explore the effects of contrastive learning from larger, less-curated image datasets such as YFCC, and find there is indeed a large difference in the resulting representation quality. We hypothesize that this curation gap is due to a shift in the distribution of image classes -- which is more diverse and heavy-tailed -- resulting in less relevant negative samples to learn from. We test this hypothesis with a new approach, Divide and Contrast (DnC), which alternates between contrastive learning and clustering-based hard negative mining. When pretrained on less curated datasets, DnC greatly improves the performance of self-supervised learning on downstream tasks, while remaining competitive with the current state-of-the-art on curated datasets.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tian","given":"Yonglong"},{"family":"Henaff","given":"Olivier J."},{"family":"Oord","given":"Aaron","dropping-particle":"van den"}],"citation-key":"tianDivideContrastSelfsupervised2021","container-title":"arXiv:2105.08054 [cs]","issued":{"date-parts":[[2021,5,17]]},"source":"arXiv.org","title":"Divide and Contrast: Self-supervised Learning from Uncurated Data","title-short":"Divide and Contrast","type":"article-journal","URL":"http://arxiv.org/abs/2105.08054"},
  {"id":"wuUnsupervisedFeatureLearning2018","abstract":"Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Wu","given":"Zhirong"},{"family":"Xiong","given":"Yuanjun"},{"family":"Yu","given":"Stella"},{"family":"Lin","given":"Dahua"}],"citation-key":"wuUnsupervisedFeatureLearning2018","container-title":"arXiv:1805.01978 [cs]","issued":{"date-parts":[[2018,5,4]]},"source":"arXiv.org","title":"Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination","type":"article-journal","URL":"http://arxiv.org/abs/1805.01978"}
]
