[
  {"id":"aroraTheoreticalAnalysisContrastive2019","abstract":"Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically \"similar\" data points and \"negative samples,\" the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Arora","given":"Sanjeev"},{"family":"Khandeparkar","given":"Hrishikesh"},{"family":"Khodak","given":"Mikhail"},{"family":"Plevrakis","given":"Orestis"},{"family":"Saunshi","given":"Nikunj"}],"citation-key":"aroraTheoreticalAnalysisContrastive2019","container-title":"arXiv:1902.09229 [cs, stat]","issued":{"date-parts":[[2019,2,25]]},"source":"arXiv.org","title":"A Theoretical Analysis of Contrastive Unsupervised Representation Learning","type":"article-journal","URL":"http://arxiv.org/abs/1902.09229"},
  {"id":"bachmanLearningRepresentationsMaximizing2019","abstract":"We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12% and concurrent results by 7%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Bachman","given":"Philip"},{"family":"Hjelm","given":"R. Devon"},{"family":"Buchwalter","given":"William"}],"citation-key":"bachmanLearningRepresentationsMaximizing2019","container-title":"arXiv:1906.00910 [cs, stat]","issued":{"date-parts":[[2019,7,8]]},"source":"arXiv.org","title":"Learning Representations by Maximizing Mutual Information Across Views","type":"article-journal","URL":"http://arxiv.org/abs/1906.00910"},
  {"id":"chenSimpleFrameworkContrastive2020a","abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenSimpleFrameworkContrastive2020a","container-title":"arXiv:2002.05709 [cs, stat]","issued":{"date-parts":[[2020,6,30]]},"source":"arXiv.org","title":"A Simple Framework for Contrastive Learning of Visual Representations","type":"article-journal","URL":"http://arxiv.org/abs/2002.05709"},
  {"id":"heMomentumContrastUnsupervised2020","abstract":"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"He","given":"Kaiming"},{"family":"Fan","given":"Haoqi"},{"family":"Wu","given":"Yuxin"},{"family":"Xie","given":"Saining"},{"family":"Girshick","given":"Ross"}],"citation-key":"heMomentumContrastUnsupervised2020","container-title":"arXiv:1911.05722 [cs]","issued":{"date-parts":[[2020,3,23]]},"source":"arXiv.org","title":"Momentum Contrast for Unsupervised Visual Representation Learning","type":"article-journal","URL":"http://arxiv.org/abs/1911.05722"},
  {"id":"henaffDataEfficientImageRecognition2020","abstract":"Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"HÃ©naff","given":"Olivier J."},{"family":"Srinivas","given":"Aravind"},{"family":"De Fauw","given":"Jeffrey"},{"family":"Razavi","given":"Ali"},{"family":"Doersch","given":"Carl"},{"family":"Eslami","given":"S. M. Ali"},{"family":"Oord","given":"Aaron","dropping-particle":"van den"}],"citation-key":"henaffDataEfficientImageRecognition2020","container-title":"arXiv:1905.09272 [cs]","issued":{"date-parts":[[2020,7,1]]},"source":"arXiv.org","title":"Data-Efficient Image Recognition with Contrastive Predictive Coding","type":"article-journal","URL":"http://arxiv.org/abs/1905.09272"},
  {"id":"hjelmLearningDeepRepresentations2019a","abstract":"In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Hjelm","given":"R. Devon"},{"family":"Fedorov","given":"Alex"},{"family":"Lavoie-Marchildon","given":"Samuel"},{"family":"Grewal","given":"Karan"},{"family":"Bachman","given":"Phil"},{"family":"Trischler","given":"Adam"},{"family":"Bengio","given":"Yoshua"}],"citation-key":"hjelmLearningDeepRepresentations2019a","container-title":"arXiv:1808.06670 [cs, stat]","issued":{"date-parts":[[2019,2,22]]},"source":"arXiv.org","title":"Learning deep representations by mutual information estimation and maximization","type":"article-journal","URL":"http://arxiv.org/abs/1808.06670"},
  {"id":"le-khacContrastiveRepresentationLearning2020","abstract":"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Le-Khac","given":"Phuc H."},{"family":"Healy","given":"Graham"},{"family":"Smeaton","given":"Alan F."}],"citation-key":"le-khacContrastiveRepresentationLearning2020","container-title":"IEEE Access","container-title-short":"IEEE Access","DOI":"10.1109/ACCESS.2020.3031549","ISSN":"2169-3536","issued":{"date-parts":[[2020]]},"page":"193907-193934","source":"arXiv.org","title":"Contrastive Representation Learning: A Framework and Review","title-short":"Contrastive Representation Learning","type":"article-journal","URL":"http://arxiv.org/abs/2010.05113","volume":"8"},
  {"id":"purushwalkamDemystifyingContrastiveSelfSupervised2020","abstract":"Self-supervised representation learning approaches have recently surpassed their supervised learning counterparts on downstream tasks like object detection and image classification. Somewhat mysteriously the recent gains in performance come from training instance classification models, treating each image and it's augmented versions as samples of a single class. In this work, we first present quantitative experiments to demystify these gains. We demonstrate that approaches like MOCO and PIRL learn occlusion-invariant representations. However, they fail to capture viewpoint and category instance invariance which are crucial components for object recognition. Second, we demonstrate that these approaches obtain further gains from access to a clean object-centric training dataset like Imagenet. Finally, we propose an approach to leverage unstructured videos to learn representations that possess higher viewpoint invariance. Our results show that the learned representations outperform MOCOv2 trained on the same data in terms of invariances encoded and the performance on downstream image classification and semantic segmentation tasks.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Purushwalkam","given":"Senthil"},{"family":"Gupta","given":"Abhinav"}],"citation-key":"purushwalkamDemystifyingContrastiveSelfSupervised2020","container-title":"arXiv:2007.13916 [cs]","issued":{"date-parts":[[2020,7,29]]},"source":"arXiv.org","title":"Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases","title-short":"Demystifying Contrastive Self-Supervised Learning","type":"article-journal","URL":"http://arxiv.org/abs/2007.13916"},
  {"id":"tianContrastiveMultiviewCoding2020","abstract":"Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a \"dog\" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tian","given":"Yonglong"},{"family":"Krishnan","given":"Dilip"},{"family":"Isola","given":"Phillip"}],"citation-key":"tianContrastiveMultiviewCoding2020","container-title":"arXiv:1906.05849 [cs]","issued":{"date-parts":[[2020,12,18]]},"source":"arXiv.org","title":"Contrastive Multiview Coding","type":"article-journal","URL":"http://arxiv.org/abs/1906.05849"},
  {"id":"tianDivideContrastSelfsupervised2021","abstract":"Self-supervised learning holds promise in leveraging large amounts of unlabeled data, however much of its progress has thus far been limited to highly curated pre-training data such as ImageNet. We explore the effects of contrastive learning from larger, less-curated image datasets such as YFCC, and find there is indeed a large difference in the resulting representation quality. We hypothesize that this curation gap is due to a shift in the distribution of image classes -- which is more diverse and heavy-tailed -- resulting in less relevant negative samples to learn from. We test this hypothesis with a new approach, Divide and Contrast (DnC), which alternates between contrastive learning and clustering-based hard negative mining. When pretrained on less curated datasets, DnC greatly improves the performance of self-supervised learning on downstream tasks, while remaining competitive with the current state-of-the-art on curated datasets.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tian","given":"Yonglong"},{"family":"Henaff","given":"Olivier J."},{"family":"Oord","given":"Aaron","dropping-particle":"van den"}],"citation-key":"tianDivideContrastSelfsupervised2021","container-title":"arXiv:2105.08054 [cs]","issued":{"date-parts":[[2021,5,17]]},"source":"arXiv.org","title":"Divide and Contrast: Self-supervised Learning from Uncurated Data","title-short":"Divide and Contrast","type":"article-journal","URL":"http://arxiv.org/abs/2105.08054"},
  {"id":"tianWhatMakesGood2020","abstract":"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tian","given":"Yonglong"},{"family":"Sun","given":"Chen"},{"family":"Poole","given":"Ben"},{"family":"Krishnan","given":"Dilip"},{"family":"Schmid","given":"Cordelia"},{"family":"Isola","given":"Phillip"}],"citation-key":"tianWhatMakesGood2020","container-title":"arXiv:2005.10243 [cs]","issued":{"date-parts":[[2020,12,18]]},"source":"arXiv.org","title":"What Makes for Good Views for Contrastive Learning?","type":"article-journal","URL":"http://arxiv.org/abs/2005.10243"},
  {"id":"toshContrastiveEstimationReveals2020","abstract":"Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tosh","given":"Christopher"},{"family":"Krishnamurthy","given":"Akshay"},{"family":"Hsu","given":"Daniel"}],"citation-key":"toshContrastiveEstimationReveals2020","container-title":"arXiv:2003.02234 [cs, stat]","issued":{"date-parts":[[2020,3,4]]},"source":"arXiv.org","title":"Contrastive estimation reveals topic posterior information to linear models","type":"article-journal","URL":"http://arxiv.org/abs/2003.02234"},
  {"id":"toshContrastiveLearningMultiview2021","abstract":"Self-supervised learning is an empirically successful approach to unsupervised learning based on creating artificial supervised learning problems. A popular self-supervised approach to representation learning is contrastive learning, which leverages naturally occurring pairs of similar and dissimilar data points, or multiple views of the same data. This work provides a theoretical analysis of contrastive learning in the multi-view setting, where two views of each datum are available. The main result is that linear functions of the learned representations are nearly optimal on downstream prediction tasks whenever the two views provide redundant information about the label.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tosh","given":"Christopher"},{"family":"Krishnamurthy","given":"Akshay"},{"family":"Hsu","given":"Daniel"}],"citation-key":"toshContrastiveLearningMultiview2021","container-title":"arXiv:2008.10150 [cs, stat]","issued":{"date-parts":[[2021,4,14]]},"source":"arXiv.org","title":"Contrastive learning, multi-view redundancy, and linear models","type":"article-journal","URL":"http://arxiv.org/abs/2008.10150"},
  {"id":"wangUnderstandingContrastiveRepresentation2020","abstract":"Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. Project Page: https://ssnl.github.io/hypersphere Code: https://github.com/SsnL/align_uniform , https://github.com/SsnL/moco_align_uniform","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Wang","given":"Tongzhou"},{"family":"Isola","given":"Phillip"}],"citation-key":"wangUnderstandingContrastiveRepresentation2020","container-title":"arXiv:2005.10242 [cs, stat]","issued":{"date-parts":[[2020,11,10]]},"source":"arXiv.org","title":"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere","type":"article-journal","URL":"http://arxiv.org/abs/2005.10242"},
  {"id":"wuUnsupervisedFeatureLearning2018","abstract":"Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Wu","given":"Zhirong"},{"family":"Xiong","given":"Yuanjun"},{"family":"Yu","given":"Stella"},{"family":"Lin","given":"Dahua"}],"citation-key":"wuUnsupervisedFeatureLearning2018","container-title":"arXiv:1805.01978 [cs]","issued":{"date-parts":[[2018,5,4]]},"source":"arXiv.org","title":"Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination","type":"article-journal","URL":"http://arxiv.org/abs/1805.01978"}
]
