[
  {"id":"chenSimpleFrameworkContrastive2020a","abstract":"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Chen","given":"Ting"},{"family":"Kornblith","given":"Simon"},{"family":"Norouzi","given":"Mohammad"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"chenSimpleFrameworkContrastive2020a","container-title":"arXiv:2002.05709 [cs, stat]","issued":{"date-parts":[[2020,6,30]]},"source":"arXiv.org","title":"A Simple Framework for Contrastive Learning of Visual Representations","type":"article-journal","URL":"http://arxiv.org/abs/2002.05709"},
  {"id":"le-khacContrastiveRepresentationLearning2020","abstract":"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Le-Khac","given":"Phuc H."},{"family":"Healy","given":"Graham"},{"family":"Smeaton","given":"Alan F."}],"citation-key":"le-khacContrastiveRepresentationLearning2020","container-title":"IEEE Access","container-title-short":"IEEE Access","DOI":"10.1109/ACCESS.2020.3031549","ISSN":"2169-3536","issued":{"date-parts":[[2020]]},"page":"193907-193934","source":"arXiv.org","title":"Contrastive Representation Learning: A Framework and Review","title-short":"Contrastive Representation Learning","type":"article-journal","URL":"http://arxiv.org/abs/2010.05113","volume":"8"},
  {"id":"tianDivideContrastSelfsupervised2021","abstract":"Self-supervised learning holds promise in leveraging large amounts of unlabeled data, however much of its progress has thus far been limited to highly curated pre-training data such as ImageNet. We explore the effects of contrastive learning from larger, less-curated image datasets such as YFCC, and find there is indeed a large difference in the resulting representation quality. We hypothesize that this curation gap is due to a shift in the distribution of image classes -- which is more diverse and heavy-tailed -- resulting in less relevant negative samples to learn from. We test this hypothesis with a new approach, Divide and Contrast (DnC), which alternates between contrastive learning and clustering-based hard negative mining. When pretrained on less curated datasets, DnC greatly improves the performance of self-supervised learning on downstream tasks, while remaining competitive with the current state-of-the-art on curated datasets.","accessed":{"date-parts":[[2022,3,4]]},"author":[{"family":"Tian","given":"Yonglong"},{"family":"Henaff","given":"Olivier J."},{"family":"Oord","given":"Aaron","dropping-particle":"van den"}],"citation-key":"tianDivideContrastSelfsupervised2021","container-title":"arXiv:2105.08054 [cs]","issued":{"date-parts":[[2021,5,17]]},"source":"arXiv.org","title":"Divide and Contrast: Self-supervised Learning from Uncurated Data","title-short":"Divide and Contrast","type":"article-journal","URL":"http://arxiv.org/abs/2105.08054"}
]
